import imaplib
import email
from email.header import decode_header
from email.utils import parseaddr
import os
import re
import unicodedata
from docx import Document
import logging
import google.generativeai as genai
from datetime import datetime
import time
import pdfplumber
import pytesseract
import pdf2image
from PIL import Image
import json
from bson import ObjectId
from pymongo import MongoClient
from services.email_tracker_mongo import init_tracking_collection, has_processed_email, mark_email_as_processed
from services.email_utils import send_result_email
from email.utils import parsedate_to_datetime

EMAIL = "thaovuong669@gmail.com"
APP_PASSWORD = "wgam okla ffjk wxwy"
IMAP_SERVER = "imap.gmail.com"
ATTACHMENTS_DIR = "cv_files"
genai.configure(api_key="AIzaSyAVZplOCSPwJpdtnSeHfPDNstBze_gUZ2Y")

os.makedirs(ATTACHMENTS_DIR, exist_ok=True)
logging.getLogger("pdfplumber").setLevel(logging.ERROR)

POSITIVE_KEYWORDS = [
    "·ª©ng tuy·ªÉn", "apply", "job application", "cv", "resume", "xin vi·ªác", "application", "tuy·ªÉn d·ª•ng", "h·ªì s∆°", "vi·ªác l√†m", "job"
]
NEGATIVE_KEYWORDS = [
    "promotion", "advertisement", "newsletter", "qu·∫£ng c√°o", "khuy·∫øn m√£i", "sale", "spam",
    "verify", "confirmation", "password", "welcome", "no-reply", "info@", "support@", "notifications@"
]
SPAM_EMAILS = [
    "recommendations@inspire.pinterest.com",
    "no-reply@mail.englishscore.com"
]

def connect_to_mongodb():
    client = MongoClient("${import.meta.env.Mongo_connect}")
    db = client["tuyendung"]
    return db

def safe_filename(filename):
    filename = unicodedata.normalize('NFKD', filename).encode('ascii', 'ignore').decode('utf-8')
    filename = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)
    return filename

def extract_and_normalize_position(subject):
    if not subject:
        return "V·ªã tr√≠ ch∆∞a x√°c ƒë·ªãnh"

    # Chu·∫©n h√≥a vƒÉn b·∫£n: lo·∫°i b·ªè d·∫•u, chuy·ªÉn th∆∞·ªùng, thay kho·∫£ng tr·∫Øng th·ª´a
    subject = unicodedata.normalize("NFKC", subject).strip().lower()
    subject = re.sub(r"\s+", " ", subject)

    # Lo·∫°i b·ªè c√°c c·ª•m t·ª´ kh√¥ng c·∫ßn thi·∫øt
    subject = re.sub(r"(?i)(·ª©ng tuy·ªÉn v·ªã tr√≠|apply for position|v·ªã tr√≠|apply|·ª©ng tuy·ªÉn)\s*[:]*\s*", "", subject)

    # T√°ch t√™n ·ª©ng vi√™n v√† c√°c ph·∫ßn th·ª´a
    patterns = [
        r"\s*[-‚Äì]\s*.+$",           # X√≥a ph·∫ßn sau d·∫•u "-"
        r"\s*(c·ªßa|by)\s*.+$",       # X√≥a ph·∫ßn sau "c·ªßa" ho·∫∑c "by"
        r"\s+[a-zA-Z\s]+$",         # X√≥a t√™n ·ªü cu·ªëi (gi·∫£ s·ª≠ t√™n l√† chu·ªói ch·ªØ)
    ]
    for pattern in patterns:
        subject = re.sub(pattern, "", subject)

    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a v√† k√Ω t·ª± kh√¥ng mong mu·ªën
    subject = re.sub(r"[^\w\s/]", "", subject).strip()

    # √Ånh x·∫° v·ªã tr√≠ sang t√™n chu·∫©n
    mappings = {
        r"frontend|front-end|react|vue|ui|giao di·ªán|web.*developer": "L·∫≠p tr√¨nh vi√™n Frontend",
        r"python|flask|django|python|[^a-zA-Z]python[^a-zA-Z]": "L·∫≠p tr√¨nh vi√™n Python",
        r"full[- ]?stack|fullstack|backend.*frontend": "L·∫≠p tr√¨nh vi√™n Full-stack",
        r"sale|b√°n h√†ng|kinh doanh|chƒÉm s√≥c kh√°ch|t∆∞ v·∫•n": "Nh√¢n vi√™n Kinh doanh",
        r"test|qa|quality assurance|ki·ªÉm th·ª≠|selenium|junit|testng": "Chuy√™n vi√™n Ki·ªÉm th·ª≠ ph·∫ßn m·ªÅm",
        r"data analyst|ph√¢n t√≠ch d·ªØ li·ªáu|power bi|tableau|looker|google data studio": "Chuy√™n vi√™n Ph√¢n t√≠ch D·ªØ li·ªáu",
        r"an to√†n th√¥ng tin|b·∫£o m·∫≠t|security|cybersecurity|ids|ips|firewall": "K·ªπ s∆∞ An to√†n Th√¥ng tin",
        r"devops|ansible|jenkins|terraform|prometheus|grafana|cicd|ci/cd": "K·ªπ s∆∞ DevOps",
        r"data engineer|big data|spark|hadoop|etl|pipelines|x·ª≠ l√Ω d·ªØ li·ªáu": "K·ªπ s∆∞ D·ªØ li·ªáu",
        r"network|m·∫°ng|infrastructure|vpn|lan|wan|router|switch": "K·ªπ s∆∞ H·∫° t·∫ßng M·∫°ng",
        r"machine learning|ml|h·ªçc m√°y|tensorflow|pytorch": "K·ªπ s∆∞ Machine Learning",
        r"ai|tr√≠ tu·ªá nh√¢n t·∫°o|artificial intelligence": "K·ªπ s∆∞ Tr√≠ tu·ªá Nh√¢n t·∫°o",
        r"ui[/-]ux|ux[/-]ui|user experience|user interface|thi·∫øt k·∫ø giao di·ªán": "Nh√† thi·∫øt k·∫ø UI/UX",
        r"senior.*ui[/-]ux|lead.*ui[/-]ux": "Nh√† thi·∫øt k·∫ø UI/UX C·∫•p cao"
    }

    for pattern, normalized in mappings.items():
        if re.search(pattern, subject, re.IGNORECASE):
            return normalized

    return subject.title()

def select_matching_jd(vi_tri_ung_tuyen, text, collection=None):
    if collection is None:
        db = connect_to_mongodb()
        collection = db["mo_ta_cong_viec"]

    jds = list(collection.find({}))
    if not jds:
        print("‚ö†Ô∏è Kh√¥ng c√≥ JD n√†o trong database")
        return None

    vi_tri_normalized = vi_tri_ung_tuyen.lower().strip()

    # ∆Øu ti√™n exact match
    for jd in jds:
        jd_position = jd.get("vi_tri", "").lower().strip()
        if vi_tri_normalized == jd_position:
            print(f"‚úÖ √Ånh x·∫° ch√≠nh x√°c '{vi_tri_ung_tuyen}' ‚Üí JD: {jd['vi_tri']}")
            return jd

    # N·∫øu kh√¥ng c√≥ exact match, th·ª≠ kh·ªõp g·∫ßn ƒë√∫ng
    print(f"‚ö†Ô∏è Kh√¥ng c√≥ JD exact match cho: {vi_tri_ung_tuyen}, th·ª≠ kh·ªõp g·∫ßn ƒë√∫ng...")
    best_jd, best_score = None, 0
    tech_keywords = [
        "reactjs", "redux", "tailwindcss", "css", "html", "javascript", "typescript",
        "node.js", "python", "mongodb", "postgresql", "mysql", "graphql", "rest", "api",
        "nessus", "owasp", "siem", "firewall", "ids", "ips", "wireshark", "metasploit",
        "ceh", "cissp", "ci/cd", "jwt", "docker", "kubernetes", "git"
    ]

    for jd_doc in jds:
        jd_position = jd_doc.get("vi_tri", "").lower().strip()
        jd_requirements = jd_doc.get("yeu_cau", "").lower()

        position_score = 0
        if vi_tri_normalized in jd_position or jd_position in vi_tri_normalized:
            position_score = 40
        elif any(keyword in vi_tri_normalized for keyword in jd_position.split()):
            position_score = 20

        jd_skills = set()
        for skill in tech_keywords:
            if skill in jd_requirements:
                jd_skills.add(skill)

        cv_skills = set()
        for word in text.lower().split():
            if word in tech_keywords:
                cv_skills.add(word)

        common_skills = cv_skills.intersection(jd_skills)
        skill_score = len(common_skills) / max(len(jd_skills), 1) * 50

        total_score = position_score + skill_score

        if total_score > best_score:
            best_jd = jd_doc
            best_score = total_score

    if best_jd and best_score >= 60:  # TƒÉng ng∆∞·ª°ng ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªô ph√π h·ª£p
        print(f"‚úÖ √Ånh x·∫° '{vi_tri_ung_tuyen}' sang JD: {best_jd['vi_tri']} (score: {best_score:.1f}%)")
        return best_jd

    print(f"‚ùå Kh√¥ng t√¨m th·∫•y JD ph√π h·ª£p cho v·ªã tr√≠: {vi_tri_ung_tuyen} (score: {best_score:.1f}%)")
    return None
def safe_get(info, key):
    return (info.get(key) or "").strip()

def pdf_to_text(pdf_path):
    try:
        with pdfplumber.open(pdf_path) as pdf:
            text = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    page_text = unicodedata.normalize("NFKC", page_text)
                    page_text = re.sub(r"[^\x00-\x7F\u00C0-\u1EF9\n\s]", " ", page_text)
                    text += page_text + "\n"
            if text.strip():
                return text.strip()
    except Exception as e:
        print(f"‚ùå L·ªói pdfplumber: {e}")

    try:
        print("üì∏ ƒêang chuy·ªÉn PDF sang ·∫£nh ƒë·ªÉ OCR...")
        images = pdf2image.convert_from_path(pdf_path)
        text = ""
        for image in images:
            ocr_result = pytesseract.image_to_string(image, lang='vie+eng')
            ocr_result = unicodedata.normalize("NFKC", ocr_result)
            ocr_result = re.sub(r"[^\x00-\x7F\u00C0-\u1EF9\n\s]", " ", ocr_result)
            text += ocr_result + "\n"
        return text.strip()
    except Exception as e:
        print(f"‚ùå L·ªói OCR fallback: {e}")
        return ""

def docx_to_text(docx_path):
    try:
        doc = Document(docx_path)
        text_parts = []
        for para in doc.paragraphs:
            if para.text.strip():
                text_parts.append(para.text.strip())
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    cell_text = cell.text.strip()
                    if cell_text:
                        text_parts.append(cell_text)
        return "\n".join(text_parts).strip()
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc DOCX {docx_path}: {e}")
        return ""

def is_recruitment_email(subject: str, from_email: str):
    subject = (subject or "").lower()
    sender = (from_email or "").lower()

    if sender in SPAM_EMAILS or any(bad in sender for bad in NEGATIVE_KEYWORDS):
        return False
    return (
        any(keyword in subject for keyword in POSITIVE_KEYWORDS)
        and not any(keyword in subject for keyword in NEGATIVE_KEYWORDS)
    )

def decode_filename(raw_filename):
    if not raw_filename:
        return None
    decoded_parts = decode_header(raw_filename)
    filename = ""
    for part, encoding in decoded_parts:
        if isinstance(part, bytes):
            filename += part.decode(encoding or "utf-8", errors="ignore")
        else:
            filename += part
    return filename

def normalize_text(text):
    if not text:
        return ""
    # Ch·ªâ l√†m s·∫°ch nh·∫π, gi·ªØ d·∫•u v√† t·ª´ kh√≥a
    text = text.strip()
    text = re.sub(r"\s+", " ", text).lower()
    return text
def extract_vi_tri_ung_tuyen(subject, text, jd_collection):
    jd_list = list(jd_collection.find({}, {"vi_tri": 1}))
    jd_positions = [normalize_text(jd["vi_tri"]) for jd in jd_list]
    jd_lookup = {normalize_text(jd["vi_tri"]): jd for jd in jd_list}

    match = re.search(r"(?i)·ª©ng tuy·ªÉn v·ªã tr√≠[:\s]+([^-‚Äì\n]+)", subject)
    if match:
        subject_vi_tri = normalize_text(match.group(1))
        print(f"üîé V·ªã tr√≠ t·ª´ subject: {subject_vi_tri}")

        if subject_vi_tri in normalize_text(text):
            print(f"‚úÖ Subject xu·∫•t hi·ªán trong CV: {subject_vi_tri}")
            if subject_vi_tri in jd_lookup:
                return subject_vi_tri
            else:
                print("‚ö†Ô∏è Subject xu·∫•t hi·ªán trong CV nh∆∞ng kh√¥ng c√≥ JD kh·ªõp")
                return subject_vi_tri

    for jd_pos in jd_positions:
        if jd_pos in normalize_text(text):
            print(f"‚úÖ T√¨m th·∫•y JD trong n·ªôi dung CV: {jd_pos}")
            return jd_pos

    for jd_pos in jd_positions:
        if jd_pos in normalize_text(subject):
            print(f"‚ö†Ô∏è Subject g·∫ßn gi·ªëng JD: {jd_pos}")
            return jd_pos

    print("‚ùå Kh√¥ng t√¨m th·∫•y JD ch√≠nh x√°c, ƒë·ªÉ Gemini t·ª± ƒëo√°n")
    return None
def extract_and_normalize_position(subject):
    if not subject:
        return "V·ªã tr√≠ ch∆∞a x√°c ƒë·ªãnh"

    # Ch·ªâ l√†m s·∫°ch nh·∫π: gi·ªØ nguy√™n d·∫•u v√† t·ª´ kh√≥a, ch·ªâ x√≥a ti·ªÅn t·ªë
    subject = subject.strip()
    
    # X√≥a ti·ªÅn t·ªë nh∆∞ "·ª©ng tuy·ªÉn v·ªã tr√≠", "apply for position"
    prefixes = r"(?i)(·ª©ng tuy·ªÉn v·ªã tr√≠|apply for position|v·ªã tr√≠|apply|·ª©ng tuy·ªÉn)\s*[:]*\s*"
    subject = re.sub(prefixes, "", subject, flags=re.IGNORECASE)

    # X√≥a t√™n ·ª©ng vi√™n ·ªü cu·ªëi (n·∫øu c√≥ d·∫•u g·∫°ch ngang ho·∫∑c "c·ªßa/by")
    name_pattern = r"\s*[-‚Äì]\s*[a-zA-Z\s√†√°√£·∫°·∫£ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠ƒ©·ªâ·ªã√≤√≥√µ·ªç·ªè√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫≈©·ª•·ªß∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ªπ·ª∑·ªµ]+$"  # Ch·ªâ kh·ªõp t√™n ti·∫øng Vi·ªát
    subject = re.sub(name_pattern, "", subject)
    
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    subject = re.sub(r"\s+", " ", subject).strip()

    # N·∫øu ti√™u ƒë·ªÅ qu√° ng·∫Øn ho·∫∑c generic, tr·∫£ v·ªÅ fallback
    if len(subject.split()) <= 1 or subject.lower() in ["k·ªπ s∆∞", "ky su"]:
        return "V·ªã tr√≠ ch∆∞a x√°c ƒë·ªãnh"

    # √Ånh x·∫° tr·ª±c ti·∫øp v·ªõi JD
    mappings = {
        r"frontend|front-end|react|vue|ui|giao di·ªán|web.*developer": "L·∫≠p tr√¨nh vi√™n Front-end",
        r"python|flask|django|fastapi": "L·∫≠p tr√¨nh vi√™n Python",
        r"full[- ]?stack|fullstack|backend.*frontend": "L·∫≠p tr√¨nh vi√™n Full-stack",
        r"sale|b√°n h√†ng|kinh doanh|chƒÉm s√≥c kh√°ch|t∆∞ v·∫•n": "Nh√¢n vi√™n Kinh doanh",
        r"test|qa|quality assurance|ki·ªÉm th·ª≠|selenium|junit|testng": "Chuy√™n vi√™n Ki·ªÉm th·ª≠ ph·∫ßn m·ªÅm",
        r"data analyst|ph√¢n t√≠ch d·ªØ li·ªáu|power bi|tableau": "Chuy√™n vi√™n Ph√¢n t√≠ch D·ªØ li·ªáu",
        r"an to√†n th√¥ng tin|b·∫£o m·∫≠t|security|cybersecurity|ids|ips|firewall": "K·ªπ s∆∞ An to√†n Th√¥ng tin",
        r"devops|ansible|jenkins|terraform|prometheus|grafana|cicd|ci/cd": "K·ªπ s∆∞ DevOps",
        r"data engineer|big data|spark|hadoop|etl|pipelines|x·ª≠ l√Ω d·ªØ li·ªáu": "K·ªπ s∆∞ D·ªØ li·ªáu",
        r"network|m·∫°ng|infrastructure|vpn|lan|wan|router|switch": "K·ªπ s∆∞ H·∫° t·∫ßng M·∫°ng",
        r"machine learning|ml|h·ªçc m√°y|tensorflow|pytorch": "K·ªπ s∆∞ Machine Learning",
        r"ai|tr√≠ tu·ªá nh√¢n t·∫°o|artificial intelligence": "K·ªπ s∆∞ Tr√≠ tu·ªá Nh√¢n t·∫°o"
    }

    for pattern, normalized in mappings.items():
        if re.search(pattern, subject, re.IGNORECASE):
            return normalized

    # N·∫øu kh√¥ng kh·ªõp v·ªõi mappings, tr·∫£ v·ªÅ ti√™u ƒë·ªÅ g·ªëc ƒë√£ l√†m s·∫°ch, vi·∫øt hoa ƒë·∫ßu t·ª´
    return subject.title()
def extract_position_and_name_from_subject(subject: str):
    subject = normalize_text(subject)

    pos_match = re.search(r"(?:ung tuyen.*?vi tri|apply for position)\s+(.+?)(?:[-‚Äì]|cua|by)", subject)
    position = pos_match.group(1).strip() if pos_match else ""

    name_match = re.search(r"(?:[-‚Äì]|cua|by)\s+(.+)$", subject)
    name = name_match.group(1).strip().title() if name_match else ""

    return position, name

def extract_info_with_gemini(text, filename, subject, jd):
    if not jd:
        print("‚ö†Ô∏è Kh√¥ng c√≥ JD ph√π h·ª£p ‚Äî ƒë·ªÉ Gemini t·ª± ph√¢n t√≠ch v·ªã tr√≠")
        jd = {
            "vi_tri": "Kh√¥ng x√°c ƒë·ªãnh",
            "mo_ta": "Kh√¥ng c√≥ m√¥ t·∫£ c·ª• th·ªÉ",
            "yeu_cau": "Kh√¥ng r√µ y√™u c·∫ßu"
        }
        fallback_mode = True
    else:
        fallback_mode = False

    def safe_string(value):
        if not isinstance(value, str):
            value = str(value)
        return value.replace("{", "{{").replace("}", "}}").strip()

    vi_tri = safe_string(jd.get("vi_tri", ""))
    mo_ta = safe_string(jd.get("mo_ta", ""))
    yeu_cau = safe_string(jd.get("yeu_cau", ""))

    jd_section = f"""
- Ki·ªÉm tra xem v·ªã tr√≠ ·ª©ng tuy·ªÉn (sau khi chu·∫©n h√≥a) c√≥ kh·ªõp v·ªõi JD ('{vi_tri.lower()}').
- So s√°nh k·ªπ nƒÉng, kinh nghi·ªám, ch·ª©ng ch·ªâ, gi·∫£i th∆∞·ªüng, d·ª± √°n, tr√¨nh ƒë·ªô h·ªçc v·∫•n v·ªõi JD:

[M√¥ t·∫£ c√¥ng vi·ªác]
- V·ªã tr√≠: {vi_tri}
- M√¥ t·∫£: {mo_ta}
- Y√™u c·∫ßu: {yeu_cau}

**H∆∞·ªõng d·∫´n so s√°nh v√† t√≠nh ƒëi·ªÉm**:
1. **K·ªπ nƒÉng (40%)**: ƒê·∫øm s·ªë k·ªπ nƒÉng trong CV kh·ªõp v·ªõi `yeu_cau`. T√≠nh t·ª∑ l·ªá k·ªπ nƒÉng kh·ªõp (s·ªë k·ªπ nƒÉng kh·ªõp / t·ªïng k·ªπ nƒÉng y√™u c·∫ßu) v√† nh√¢n v·ªõi 40.
2. **Kinh nghi·ªám (30%)**: ƒê√°nh gi√° s·ªë nƒÉm kinh nghi·ªám ho·∫∑c s·ªë d·ª± √°n li√™n quan. N·∫øu kinh nghi·ªám >= y√™u c·∫ßu, cho 30 ƒëi·ªÉm; n·∫øu ƒë·∫°t 50‚Äì99% y√™u c·∫ßu, cho 15‚Äì29 ƒëi·ªÉm; n·∫øu <50%, cho 0‚Äì14 ƒëi·ªÉm.
3. **H·ªçc v·∫•n v√† ch·ª©ng ch·ªâ (20%)**: N·∫øu tr√¨nh ƒë·ªô h·ªçc v·∫•n ho·∫∑c ch·ª©ng ch·ªâ kh·ªõp v·ªõi y√™u c·∫ßu, cho 20 ƒëi·ªÉm; n·∫øu thi·∫øu m·ªôt ph·∫ßn, cho 0‚Äì19 ƒëi·ªÉm.
4. **Kh√°c (10%)**: D·ª± √°n, gi·∫£i th∆∞·ªüng ho·∫∑c c√°c y·∫øu t·ªë kh√°c li√™n quan ƒë·∫øn JD, t·ªëi ƒëa 10 ƒëi·ªÉm.
5. T·ªïng ƒëi·ªÉm (`diem_phu_hop`) t·ª´ 0‚Äì100
6. N·∫øu v·ªã tr√≠ ·ª©ng tuy·ªÉn kh√¥ng kh·ªõp v·ªõi JD, tr·∫£ v·ªÅ `nhan_xet`: "Hi·ªán kh√¥ng c√≥ JD ph√π h·ª£p v·ªõi v·ªã tr√≠ n√†y" v√† `diem_phu_hop` t·ªëi ƒëa 50.
"""

    prompt = f"""
B·∫°n l√† AI chuy√™n gia tuy·ªÉn d·ª•ng. D·ª±a tr√™n CV v√† JD, h√£y tr√≠ch xu·∫•t th√¥ng tin theo m·∫´u JSON sau v√† ƒë√°nh gi√° m·ª©c ƒë·ªô ph√π h·ª£p.

**VƒÉn b·∫£n CV**:
{text}

**Tr·∫£ v·ªÅ JSON chu·∫©n sau**:
{{
  "ho_ten": "",
  "so_dien_thoai": "",
  "ngay_sinh": "",
  "que_quan": "",
  "noi_o": "",
  "trinh_do_hoc_van": [],
  "kinh_nghiem": [],
  "vi_tri_ung_tuyen": "",
  "ky_nang": [],
  "chung_chi": [],
  "giai_thuong": [],
  "du_an": [],
  "trang_thai": "",
  "diem_phu_hop": 0,
  "nhan_xet": "",
  "muc_tieu_nghe_nghiep": "",
  "so_thich": [],
  "nguoi_gioi_thieu": [],
  "hoat_dong": []
}}

    **Y√™u c·∫ßu ƒë·ªãnh d·∫°ng**:
    - C√°c tr∆∞·ªùng `trinh_do_hoc_van`, `kinh_nghiem`, `ky_nang`, `chung_chi`, `giai_thuong`, `du_an`, `so_thich`, `nguoi_gioi_thieu`, `hoat_dong` ph·∫£i l√† danh s√°ch c√°c CHU·ªñI (string), KH√îNG ph·∫£i object.
    - **Chi ti·∫øt y√™u c·∫ßu**:
      - `trinh_do_hoc_van`: L·∫•y b·∫±ng c·∫•p, chuy√™n ng√†nh, tr∆∞·ªùng v√† nƒÉm t·ªët nghi·ªáp (n·∫øu c√≥). V√≠ d·ª•: ["C·ª≠ nh√¢n CNTT ƒê·∫°i h·ªçc B√°ch khoa H√† N·ªôi 2010"].
      - `kinh_nghiem`: L·∫•y v·ªã tr√≠ c√¥ng vi·ªác, c√¥ng ty, th·ªùi gian v√† m√¥ t·∫£ ng·∫Øn g·ªçn nhi·ªám v·ª•. V√≠ d·ª•: ["Network Engineer t·∫°i Viettel 2018-2020: C·∫•u h√¨nh router Cisco"].
      - `ky_nang`: L·∫•y c√°c k·ªπ nƒÉng k·ªπ thu·∫≠t ho·∫∑c m·ªÅm. V√≠ d·ª•: ["Python", "Teamwork"].
      - `chung_chi`: L·∫•y t√™n ch·ª©ng ch·ªâ, nƒÉm v√† t·ªï ch·ª©c c·∫•p (n·∫øu c√≥). V√≠ d·ª•: ["TOEIC 900 2023", "AWS Certified 2022"].
      - `giai_thuong`: L·∫•y t√™n gi·∫£i th∆∞·ªüng, nƒÉm v√† t·ªï ch·ª©c/s·ª± ki·ªán. V√≠ d·ª•: ["Gi·∫£i nh·∫•t Hackathon 2023"].
      - `du_an`: L·∫•y t√™n d·ª± √°n, c√¥ng ty, nƒÉm v√† m√¥ t·∫£ ng·∫Øn g·ªçn nhi·ªám v·ª•, g·ªôp th√†nh m·ªôt chu·ªói. V√≠ d·ª•: ["X√¢y d·ª±ng m·∫°ng n·ªôi b·ªô cho trung t√¢m d·ªØ li·ªáu t·∫°i IDC Vi·ªát Nam 2023: C·∫•u h√¨nh BGP, OSPF, Firewall ƒëa l·ªõp"].
      - `so_thich`: L·∫•y s·ªü th√≠ch c√° nh√¢n. V√≠ d·ª•: ["T·∫≠p gym", "Tham gia hackathon"].
      - `nguoi_gioi_thieu`: L·∫•y t√™n, v·ªã tr√≠, c√¥ng ty, email v√† s·ªë ƒëi·ªán tho·∫°i (n·∫øu c√≥). V√≠ d·ª•: ["Nguy·ªÖn VƒÉn A - Manager t·∫°i Viettel - a@viettel.vn - 0123456789"].
      - `hoat_dong`: L·∫•y vai tr√≤, t·ªï ch·ª©c, th·ªùi gian v√† m√¥ t·∫£ ng·∫Øn g·ªçn. V√≠ d·ª•: ["Tr∆∞·ªüng ban C√¢u l·∫°c b·ªô Kh·ªüi nghi·ªáp 2018-2020: T·ªï ch·ª©c workshop"].
    - N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, ƒë·ªÉ danh s√°ch r·ªóng (`[]`) ho·∫∑c chu·ªói r·ªóng (`""`).


**L∆∞u √Ω**:
- Ch·ªâ tr·∫£ v·ªÅ ƒë√∫ng JSON, kh√¥ng th√™m m√¥ t·∫£ ngo√†i.
- `diem_phu_hop` ph·∫£i t·ª´ 0‚Äì100, t√≠nh theo h∆∞·ªõng d·∫´n tr√™n.
- `nhan_xet` ph·∫£i n√™u r√µ nh·ªØng ƒëi·ªÉm CV c√≤n thi·∫øu so v·ªõi JD v√† g·ª£i √Ω c·∫£i thi·ªán. N·∫øu v·ªã tr√≠ kh√¥ng kh·ªõp, ghi "Hi·ªán kh√¥ng c√≥ JD ph√π h·ª£p v·ªõi v·ªã tr√≠ n√†y".
- Kh√¥ng ƒëi·ªÅn th√¥ng tin n·∫øu kh√¥ng c√≥ trong CV.
- C√°c tr∆∞·ªùng m·ªõi nh∆∞ `muc_tieu_nghe_nghiep`, `so_thich`, `nguoi_gioi_thieu`, `hoat_dong` ch·ªâ ƒëi·ªÅn n·∫øu c√≥ d·ªØ li·ªáu trong CV.
- ‚ö†Ô∏è L∆∞u √Ω: V√¨ kh√¥ng t√¨m th·∫•y JD ph√π h·ª£p n√™n b·∫°n ph·∫£i t·ª± x√°c ƒë·ªãnh: ƒë·∫ßu ti√™n, x√°c ƒë·ªãnh tr√™n ti√™u ƒë·ªÅ (t·ª± ƒë·ªçc ƒë·ªÉ nh·∫≠n bi·∫øt, c√≥ th·ªÉ b·ªè t·ª´ kh√¥ng li√™n qua nh∆∞ v·ªã tr√≠, ·ª©ng tuy·ªÉn, v·ªã tr√≠ ·ª©ng tuy·ªÉn ho·∫∑c c·ªßa, T√™n ng∆∞·ªùi g·ª≠i). 
N·∫øu l·∫•y ƒë∆∞·ª£c v·ªã tr√≠ ·ª©ng tuy·ªÉn ·ªü ti√™u ƒë·ªÅ m·ªõi ƒë·ªçc t·ª´ n·ªôi dung CV, xem trong 10 d√≤ng ƒë·∫ßu c√≥ c√¢u n√†o t∆∞∆°ng t·ª± v·ªõi ti√™u ƒë·ªÅ ko, n·∫øu c√≥ ƒë√≥ l√† v·ªã tr√≠ ·ª©ng tuy·ªÉn, n·∫øu kh√¥ng t·ª± suy ra t·ª´ n·ªôi dung CV (ki·ªÉm tra t·∫ßm 10 d√≤ng ƒë·∫ßu ƒë·ªÉ suy ra th√¥i). N·∫øu kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c, h√£y ƒë·ªÉ tr·ªëng tr∆∞·ªùng `vi_tri_ung_tuyen`. Trong m·ªçi tr∆∞·ªùng h·ª£p, `nhan_xet` ph·∫£i ghi r√µ 'Kh√¥ng t√¨m th·∫•y JD ph√π h·ª£p' v√† ƒëi·ªÉm t·ªëi ƒëa kh√¥ng v∆∞·ª£t qu√° 50.
- N·∫øu `ho_ten` = null . ƒê·∫ßu ti√™n b·∫°n xem l·∫°i ti√™u ƒë·ªÅ (th∆∞·ªùng th√¨ t√™n sau ch·ªØ c·ªßa hoƒÉc -), n·∫øu c√≥ th√¨ g√°n ƒë√≥ l√† `ho_ten`, n·∫øu kh√¥ng c√≥ xem x√©t ·ªü n·ªôi dung CV t·ª± ƒë·ªçc xem trong v√≤ng 10 d√≤ng ƒë·∫ßu ti√™n c√≥ ph·∫ßn n√†o n·ªïi b·∫≠t, gi·ªëng h·ªç t√™n ng∆∞·ªùi nh·∫•t th√¨ g√°n, kh√¥ng ƒë·ªÉ `ho_ten` = null


{jd_section}
"""

    try:
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt)
        print("üìú Ph·∫£n h·ªìi th√¥ t·ª´ Gemini:", response.text)

        text = response.text.strip()
        if "```" in text:
            parts = [part for part in text.split("```") if "{" in part]
            raw_json = parts[0] if parts else text
        else:
            raw_json = text

        match = re.search(r"\{.*\}", raw_json, re.DOTALL)
        if not match:
            print("‚ùå Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá trong ph·∫£n h·ªìi Gemini")
            return None

        result = json.loads(match.group())

        if "diem_phu_hop" not in result or not isinstance(result["diem_phu_hop"], (int, float)):
            print("‚ùå Gemini kh√¥ng tr·∫£ v·ªÅ tr∆∞·ªùng 'diem_phu_hop' h·ª£p l·ªá, tr·∫£ v·ªÅ gi√° tr·ªã m·∫∑c ƒë·ªãnh")
            result["diem_phu_hop"] = 0
            result["nhan_xet"] = "Kh√¥ng th·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô ph√π h·ª£p do thi·∫øu th√¥ng tin t·ª´ Gemini."

        return result

    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi Gemini: {e}")
        import traceback
        traceback.print_exc()
        return None

def process_all_emails(mongo_collection):
    tracking_collection = init_tracking_collection(mongo_collection.database)
    from_email = "Kh√¥ng c√≥ email ƒë∆∞·ª£c x·ª≠ l√Ω"
    try:
        mail = imaplib.IMAP4_SSL(IMAP_SERVER)
        mail.login(EMAIL, APP_PASSWORD)
        mail.select("inbox")

        status, messages = mail.search(None, "ALL")
        if status != "OK":
            print("‚ùå Kh√¥ng t√¨m th·∫•y email.")
            return
        print("üü¢ B·∫Øt ƒë·∫ßu qu√©t email...")

        for num in messages[0].split():
            _, msg_data = mail.fetch(num, "(RFC822)")
            msg = email.message_from_bytes(msg_data[0][1])
            uid = msg["Message-ID"] or num.decode()
            
            email_date_raw = msg["Date"]
            try:
                email_date = parsedate_to_datetime(email_date_raw)
            except:
                email_date = datetime.now()

            print("üìå UID c·ªßa email:", uid)
            print("üîç T√¨m trong tracking:", tracking_collection.find_one({"uid": uid}))

            subject = decode_header(msg.get("Subject") or "")[0][0]
            if isinstance(subject, bytes):
                subject = subject.decode(errors="ignore")
            from_email = parseaddr(msg.get("From"))[1]

            print(f"üì® Email: {subject} -- From: {from_email}")
            print("üîé is_recruitment_email:", is_recruitment_email(subject, from_email))
            print("üîÑ has_processed_email:", has_processed_email(tracking_collection, uid))

            if not is_recruitment_email(subject, from_email) or has_processed_email(tracking_collection, uid):
                print("‚õî Email b·ªã b·ªè qua")
                continue

            text, filepath = "", None

            for part in msg.walk():
                if part.get_content_maintype() == "multipart":
                    continue
                if part.get("Content-Disposition") is None:
                    continue

                filename = decode_filename(part.get_filename())
                if filename and filename.lower().endswith((".pdf", ".docx")):
                    safe_name = safe_filename(filename)
                    filepath = os.path.join(ATTACHMENTS_DIR, safe_name)
                    with open(filepath, "wb") as f:
                        f.write(part.get_payload(decode=True))
                    if filename.endswith(".pdf"):
                        text = pdf_to_text(filepath)
                    else:
                        text = docx_to_text(filepath)

            if not text:
                for part in msg.walk():
                    if part.get_content_type() == "text/plain":
                        charset = part.get_content_charset() or "utf-8"
                        try:
                            text += part.get_payload(decode=True).decode(charset, errors="ignore")
                        except:
                            pass

            jd_collection = mongo_collection.database["mo_ta_cong_viec"]
            jd_docs = list(jd_collection.find({}, {"vi_tri": 1}))
            jd_positions = [normalize_text(doc["vi_tri"]) for doc in jd_docs]

           # L·∫•y v·ªã tr√≠ t·ª´ subject
            vi_tri_from_subject = extract_and_normalize_position(subject)
            print(f"üß† V·ªã tr√≠ chu·∫©n h√≥a t·ª´ subject: {vi_tri_from_subject}")

            # Ki·ªÉm tra v·ªã tr√≠ trong n·ªôi dung CV (10 d√≤ng ƒë·∫ßu)
            vi_tri_from_text = ""
            lines = text.strip().split("\n")
            jd_collection = mongo_collection.database["mo_ta_cong_viec"]
            jd_positions = [jd["vi_tri"].lower().strip() for jd in jd_collection.find({}, {"vi_tri": 1})]

            for line in lines[:10]:
                line_norm = line.lower().strip()
                for jd_pos in jd_positions:
                    if jd_pos in line_norm:
                        vi_tri_from_text = jd_pos
                        break
                if vi_tri_from_text:
                    break

            # Ch·ªçn v·ªã tr√≠ cu·ªëi c√πng
            vi_tri_chuan_norm = vi_tri_from_subject if vi_tri_from_subject != "V·ªã tr√≠ ch∆∞a x√°c ƒë·ªãnh" else vi_tri_from_text
            if not vi_tri_chuan_norm:
                vi_tri_chuan_norm = subject.strip().title()  # Fallback to cleaned subject
            print(f"üß† V·ªã tr√≠ chu·∫©n h√≥a cu·ªëi c√πng: {vi_tri_chuan_norm}")

            # T√¨m JD ph√π h·ª£p
            matched_jd = select_matching_jd(vi_tri_chuan_norm, text, jd_collection)

            info = extract_info_with_gemini(
                text,
                os.path.basename(filepath) if filepath else "no_file",
                subject,
                matched_jd
            )

            trang_thai_ung_vien = ""
            status_match = re.search(r"(?i)tr·∫°ng th√°i.*?:\s*([^\n]+)", text)
            if status_match:
                trang_thai_ung_vien = status_match.group(1).strip()
            elif "ch·ªù ph·ªèng v·∫•n" in text.lower():
                trang_thai_ung_vien = "ƒêang ·ª©ng tuy·ªÉn"
            subject_vi_tri, subject_name = extract_position_and_name_from_subject(subject)
            if not info or not info.get("ho_ten"):
                if subject_name:
                    info = info or {}
                    info["ho_ten"] = subject_name
                    print(f"üìù T√™n l·∫•y t·ª´ ti√™u ƒë·ªÅ: {info['ho_ten']}")
                else:
                    print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y t√™n ·ª©ng vi√™n, b·ªè qua email")
                    continue

            if not info.get("vi_tri_ung_tuyen") and subject_vi_tri:
                info["vi_tri_ung_tuyen"] = subject_vi_tri.title()
                print(f"üìå V·ªã tr√≠ ·ª©ng tuy·ªÉn fallback t·ª´ subject: {info['vi_tri_ung_tuyen']}")

            diem_phu_hop = info.get("diem_phu_hop")
            doc = {
            "ho_ten": info.get("ho_ten", ""),
            "email": from_email,
            "so_dien_thoai": safe_get(info, "so_dien_thoai"),
            "ngay_sinh": safe_get(info, "ngay_sinh"),
            "que_quan": safe_get(info, "que_quan"),
            "noi_o": safe_get(info, "noi_o"),
            "trinh_do_hoc_van": info.get("trinh_do_hoc_van", []),
            "kinh_nghiem": info.get("kinh_nghiem", []),
            "vi_tri_ung_tuyen": matched_jd["vi_tri"] if matched_jd else vi_tri_chuan_norm,
            "ky_nang": info.get("ky_nang", []),
            "chung_chi": info.get("chung_chi", []),
            "giai_thuong": info.get("giai_thuong", []),
            "du_an": info.get("du_an", []),
            "trang_thai": trang_thai_ung_vien or "ƒêang ·ª©ng tuy·ªÉn",
            "trang_thai_gui_email": "Ch∆∞a g·ª≠i",
            "ngay_nop": datetime.now(),
            "diem_phu_hop": int(diem_phu_hop or 0),
            "nhan_xet": info.get("nhan_xet", "Hi·ªán kh√¥ng c√≥ JD ph√π h·ª£p v·ªõi v·ªã tr√≠ n√†y. ƒê·ªÅ ngh·ªã xem x√©t c√°c v·ªã tr√≠ nh∆∞ K·ªπ s∆∞ Tr√≠ tu·ªá Nh√¢n t·∫°o ho·∫∑c K·ªπ s∆∞ Machine Learning." if not matched_jd else ""),
            "cv_filepath": filepath or "",
            "ngay_gui": email_date,
            "muc_tieu_nghe_nghiep": info.get("muc_tieu_nghe_nghiep", ""),
            "so_thich": info.get("so_thich", []),
            "nguoi_gioi_thieu": info.get("nguoi_gioi_thieu", []),
            "hoat_dong": info.get("hoat_dong", [])
        }

            mongo_collection.insert_one(doc)
            mark_email_as_processed(tracking_collection, uid, from_email, subject)
            print(f"‚úÖ ƒê√£ l∆∞u ·ª©ng vi√™n: {doc['ho_ten']} - {from_email} - ƒêi·ªÉm: {diem_phu_hop}")
            time.sleep(5)

        mail.logout()

    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω email: {e}")
        import traceback
        traceback.print_exc()

    print(f"‚úÖ ƒê√£ x·ª≠ l√Ω email t·ª´: {from_email}")